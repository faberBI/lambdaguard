# ============================================================
# üõ°Ô∏è XGBoost Œª-Based Early Stopping on Random 300 Rows
# ============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# -----------------------------
# 0Ô∏è‚É£ Load Data 
# -----------------------------
california = fetch_california_housing(as_frame=True)
X = california.data
y = california.target

# train/validation split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)
dtrain = xgb.DMatrix(X_train, label=y_train)
dval = xgb.DMatrix(X_val, label=y_val)

# -----------------------------
# 1Ô∏è‚É£ Model Params
# -----------------------------
params = {
    "max_depth": 10,
    "eta": 0.05,
    "subsample": 0.8,
    "objective": "reg:squarederror",
    "eval_metric": "rmse",
    "seed": 42
}
MAX_ROUNDS = 500

# -----------------------------
# 2Ô∏è‚É£ Lambda Functions
# -----------------------------
def generalization_index_Z(model, X, y):
    """
    Generalization Index (GI) universale per:
    - XGBoost (Booster e sklearn API)
    - LightGBM
    - CatBoost
    - sklearn GradientBoosting
    
    Returns:
        GI, A (alignment), C (capacity)
    """
    
    # --------------------------------------------------
    # 1Ô∏è‚É£ BUILDING (leaf_matrix)
    # --------------------------------------------------
    
    leaf_matrix = None
    preds = None
    
    # ----------------------------
    # XGBOOST - Booster nativo
    # ----------------------------
    if hasattr(model, "predict") and hasattr(model, "get_booster") is False and "xgboost" in str(type(model)).lower():
        try:
            import xgboost as xgb
            dmatrix = xgb.DMatrix(X)
            leaf_matrix = model.predict(dmatrix, pred_leaf=True)
            preds = model.predict(dmatrix)
        except:
            pass

    # ----------------------------
    # XGBOOST - sklearn API
    # ----------------------------
    if leaf_matrix is None and hasattr(model, "get_booster"):
        leaf_matrix = model.apply(X)
        preds = model.predict(X)

    # ----------------------------
    # LIGHTGBM
    # ----------------------------
    if leaf_matrix is None and "lightgbm" in str(type(model)).lower():
        leaf_matrix = model.predict(X, pred_leaf=True)
        preds = model.predict(X)

    # ----------------------------
    # CATBOOST
    # ----------------------------
    if leaf_matrix is None and "catboost" in str(type(model)).lower():
        leaf_matrix = model.calc_leaf_indexes(X)
        preds = model.predict(X)

    # ----------------------------
    # SKLEARN GradientBoosting
    # ----------------------------
    if leaf_matrix is None and hasattr(model, "estimators_"):
        leaf_list = []
        for est in model.estimators_.ravel():
            leaf_list.append(est.apply(X))
        leaf_matrix = np.column_stack(leaf_list)
        preds = model.predict(X)

    # Se ancora None ‚Üí errore
    if leaf_matrix is None:
        raise ValueError("Model unsupported")

    # --------------------------------------------------
    # 2Ô∏è‚É£ MATRIX 2D
    # --------------------------------------------------
    leaf_matrix = np.array(leaf_matrix)
    if leaf_matrix.ndim == 1:
        leaf_matrix = leaf_matrix.reshape(-1, 1)

    # --------------------------------------------------
    # 3Ô∏è‚É£ Z MATRIX
    # --------------------------------------------------
    Z_cols = []

    for t in range(leaf_matrix.shape[1]):
        leaf_ids = leaf_matrix[:, t]
        unique_leaves = np.unique(leaf_ids)

        for leaf in unique_leaves:
            Z_cols.append((leaf_ids == leaf).astype(float))

    if len(Z_cols) == 0:
        return 0, 0, 0

    Z = np.column_stack(Z_cols)

    # --------------------------------------------------
    # 4Ô∏è‚É£ CAPACITY
    # --------------------------------------------------
    C = np.var(Z)

    # --------------------------------------------------
    # 5Ô∏è‚É£ ALIGNMENT
    # --------------------------------------------------
    if preds is None:
        preds = model.predict(X)

    A = np.corrcoef(preds, y)[0, 1] if np.std(preds) > 0 else 0

    # --------------------------------------------------
    # 6Ô∏è‚É£ GENERALIZATION INDEX
    # --------------------------------------------------
    GI = A / C if C > 0 else 0

    return GI, A, C

def compute_lambda(model, X_ref, y_ref, noise_std=1e-3):

    # -------------------------
    # XGBoost Booster
    # -------------------------
    if isinstance(model, xgb.Booster):

        d_ref = xgb.DMatrix(X_ref)
        d_ref_noisy = xgb.DMatrix(
            X_ref + np.random.normal(0, noise_std, X_ref.shape)
        )

        preds = model.predict(d_ref)
        preds_noisy = model.predict(d_ref_noisy)

    # -------------------------
    # All other models
    # -------------------------
    else:
        preds = model.predict(X_ref)
        preds_noisy = model.predict(
            X_ref + np.random.normal(0, noise_std, X_ref.shape)
        )

    GI, A, C = generalization_index_Z(model, X_ref, y_ref)

    S = np.mean(np.abs(preds - preds_noisy)) / (np.std(preds) + 1e-8)
    OFI = (C / (A + C + 1e-8)) * S

    return OFI, A

def lambda_stop_check(lambdas, A_hist, baseline_points=30, smooth_window=10, cusum_threshold_factor=2):
    if len(lambdas) < baseline_points + 3:
        return False
    lambdas = np.array(lambdas)
    mu = lambdas[:baseline_points].mean()
    sigma = lambdas[:baseline_points].std() + 1e-8
    z = (lambdas - mu) / sigma
    d1 = np.diff(z)
    d1 = pd.Series(d1).rolling(smooth_window, min_periods=1).mean().values
    d2 = np.diff(d1)
    d2 = pd.Series(d2).rolling(smooth_window, min_periods=1).mean().values
    centered = d2 - d2.mean()
    cusum = np.maximum.accumulate(np.maximum(0, centered.cumsum()))
    threshold = cusum_threshold_factor * d2.std()
    A_flat = np.abs(np.diff(A_hist[-3:])).mean() < 0.01
    return cusum[-1] > threshold and A_flat

# -----------------------------
# 3Ô∏è‚É£ Lambda-Based Training
# -----------------------------
lambda_hist = []
A_hist = []
train_rmse = []
val_rmse = []

lambda_stop_round = None
booster_lambda = None

for n_rounds in range(1, MAX_ROUNDS + 1):
    booster = xgb.train(params, dtrain, num_boost_round=n_rounds, verbose_eval=False)
    y_tr = booster.predict(dtrain)
    y_va = booster.predict(dval)
    train_rmse.append(np.sqrt(mean_squared_error(y_train, y_tr)))
    val_rmse.append(np.sqrt(mean_squared_error(y_val, y_va)))
    OFI, A = compute_lambda(booster, X_train, y_train)
    lambda_hist.append(OFI)
    A_hist.append(A)
    if lambda_stop_check(lambda_hist, A_hist):
        lambda_stop_round = n_rounds
        booster_lambda = booster
        break

# -----------------------------
# 4Ô∏è‚É£ Classic Early Stopping Loop (multiple rounds)
# -----------------------------
early_stopping_list = [1, 2, 3, 5, 10, 20, 30, 50, 70, 100]
results_classic = []

for es_rounds in early_stopping_list:
    booster_tmp = xgb.train(
        params=params,
        dtrain=dtrain,
        num_boost_round=MAX_ROUNDS,
        evals=[(dtrain,"train"), (dval,"val")],
        early_stopping_rounds=es_rounds,
        verbose_eval=False
    )
    best_iter = booster_tmp.best_iteration
    train_rmse_val = np.sqrt(mean_squared_error(y_train, booster_tmp.predict(dtrain)))
    val_rmse_val = np.sqrt(mean_squared_error(y_val, booster_tmp.predict(dval)))
    gap = val_rmse_val - train_rmse_val
    results_classic.append({
        "early_stopping_rounds": es_rounds,
        "best_iteration": best_iter,
        "train_rmse": train_rmse_val,
        "val_rmse": val_rmse_val,
        "gap": gap
    })

df_classic = pd.DataFrame(results_classic)

# -----------------------------
# 5Ô∏è‚É£ Plot curves (lambda + RMSE full)
# -----------------------------
# full train to MAX_ROUNDS
train_rmse_full = [np.sqrt(mean_squared_error(y_train, booster.predict(dtrain, iteration_range=(0,i+1)))) for i in range(MAX_ROUNDS)]
val_rmse_full = [np.sqrt(mean_squared_error(y_val, booster.predict(dval, iteration_range=(0,i+1)))) for i in range(MAX_ROUNDS)]

lambdas = np.array(lambda_hist)
mu = lambdas[:5].mean()
sigma = lambdas[:5].std() + 1e-8
lambda_z = (lambdas - mu)/sigma
d1 = np.diff(lambda_z)
d1 = pd.Series(d1).rolling(3, min_periods=1).mean().values
d2 = np.diff(d1)
d2 = pd.Series(d2).rolling(3, min_periods=1).mean().values
iters = np.arange(1, MAX_ROUNDS+1)

fig, ax1 = plt.subplots(figsize=(12,6))
ax1.plot(iters[:len(lambda_z)], lambda_z, label="Œª (normalized)", linewidth=2)
ax1.axvline(lambda_stop_round, linestyle="--", label="Œª stop", color='red')
for es_row in df_classic.itertuples():
    ax1.axvline(es_row.best_iteration, linestyle=":", label=f"classic stop {es_row.early_stopping_rounds}", alpha=0.7)
ax1.set_xlabel("Boosting rounds")
ax1.set_ylabel("Œª (z-score)")
ax1.grid(True)

ax2 = ax1.twinx()
ax2.plot(iters, train_rmse_full, alpha=0.6, label="Train RMSE (full)")
ax2.plot(iters, val_rmse_full, alpha=0.6, label="Val RMSE (full)")
ax2.plot(iters[2:len(d2)+2], d2, linestyle=":", linewidth=2, label="d¬≤Œª")
ax2.set_ylabel("d¬≤Œª / RMSE")

lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines1 + lines2, labels1 + labels2, loc="best")
plt.title("Œª-based vs Classic Early Stopping (300 rows sample)")
plt.tight_layout()
plt.show()

# -----------------------------
# 6Ô∏è‚É£ Summary Table
# -----------------------------
lambda_train_rmse = train_rmse[lambda_stop_round-1]
lambda_val_rmse = val_rmse[lambda_stop_round-1]
lambda_gap = lambda_val_rmse - lambda_train_rmse

comparison = pd.DataFrame({
    "Metric": ["Stop round","Train RMSE","Val RMSE","Generalization gap (val-train)"],
    "Lambda stopping": [lambda_stop_round, lambda_train_rmse, lambda_val_rmse, lambda_gap],
    "Classic stopping (best from loop)": [
        df_classic.loc[df_classic['val_rmse'].idxmin(),'best_iteration'],
        df_classic.loc[df_classic['val_rmse'].idxmin(),'train_rmse'],
        df_classic.loc[df_classic['val_rmse'].idxmin(),'val_rmse'],
        df_classic.loc[df_classic['val_rmse'].idxmin(),'gap']
    ]
})

print("\nCOMPARISON TABLE")
display(comparison)
